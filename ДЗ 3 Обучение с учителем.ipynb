{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9db9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Обучение с учителем\n",
    "#Задание 1\n",
    "#Импортируйте библиотеки pandas и numpy.\n",
    "\n",
    "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и Y из этих данных.\n",
    "\n",
    "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.\n",
    "\n",
    "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
    "\n",
    "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.\n",
    "\n",
    "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "data = boston[\"data\"]\n",
    "feature_names = boston[\"feature_names\"]\n",
    "\n",
    "X = pd.DataFrame(data, columns=feature_names)\n",
    "X.head()\n",
    "CRIM    ZN  INDUS   CHAS    NOX RM  AGE DIS RAD TAX PTRATIO B   LSTAT\n",
    "0   0.00632 18.0    2.31    0.0 0.538   6.575   65.2    4.0900  1.0 296.0   15.3    396.90  4.98\n",
    "1   0.02731 0.0 7.07    0.0 0.469   6.421   78.9    4.9671  2.0 242.0   17.8    396.90  9.14\n",
    "2   0.02729 0.0 7.07    0.0 0.469   7.185   61.1    4.9671  2.0 242.0   17.8    392.83  4.03\n",
    "3   0.03237 0.0 2.18    0.0 0.458   6.998   45.8    6.0622  3.0 222.0   18.7    394.63  2.94\n",
    "4   0.06905 0.0 2.18    0.0 0.458   7.147   54.2    6.0622  3.0 222.0   18.7    396.90  5.33\n",
    "target = boston[\"target\"]\n",
    "\n",
    "Y = pd.DataFrame(target, columns=[\"price\"])\n",
    "Y.head()\n",
    "price\n",
    "0   24.0\n",
    "1   21.6\n",
    "2   34.7\n",
    "3   33.4\n",
    "4   36.2\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=42)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "check_test_lr = pd.DataFrame({\n",
    "    \"Y_test\": Y_test[\"price\"], \n",
    "    \"Y_pred_lr\": y_pred_lr.flatten()})\n",
    "\n",
    "check_test_lr.head()\n",
    "Y_test  Y_pred_lr\n",
    "173 23.6    28.648960\n",
    "274 32.4    36.495014\n",
    "491 13.6    15.411193\n",
    "72  22.8    25.403213\n",
    "452 16.1    18.855280\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error_lr = mean_squared_error(check_test_lr[\"Y_pred_lr\"], check_test_lr[\"Y_test\"])\n",
    "print(mean_squared_error_lr)\n",
    "21.51744423117741\n",
    "Задание 2\n",
    "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
    "\n",
    "Сделайте агрумент n_estimators равным 1000, max_depth должен быть равен 12 и random_state сделайте равным 42.\n",
    "\n",
    "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression, но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0], чтобы получить из датафрейма одномерный массив Numpy, так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.\n",
    "\n",
    "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
    "\n",
    "Напишите в комментариях к коду, какая модель в данном случае работает лучше.\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
    "clf.fit(X_train, Y_train.values[:, 0])\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12,\n",
    "                      max_features='auto', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=2,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                      n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
    "                      warm_start=False)\n",
    "y_pred_clf = clf.predict(X_test)\n",
    "check_test_clf = pd.DataFrame({\n",
    "    \"Y_test\": Y_test[\"price\"], \n",
    "    \"Y_pred_clf\": y_pred_clf.flatten()})\n",
    "\n",
    "check_test_clf.head()\n",
    "Y_test  Y_pred_clf\n",
    "173 23.6    22.846138\n",
    "274 32.4    31.156114\n",
    "491 13.6    16.297226\n",
    "72  22.8    23.821036\n",
    "452 16.1    17.212148\n",
    "mean_squared_error_clf = mean_squared_error(check_test_clf[\"Y_pred_clf\"], check_test_clf[\"Y_test\"])\n",
    "print(mean_squared_error_clf)\n",
    "9.31439570598467\n",
    "print(mean_squared_error_lr, mean_squared_error_clf)\n",
    "21.51744423117741 9.31439570598467\n",
    "Алгоритм \"Случайный лес\" показывает более точные результаты, чем \"линейная регрессия\". Примерно в 3 раза.\n",
    "\n",
    "* Задание 3\n",
    "Вызовите документацию для класса , найдите информацию об атрибуте featureimportances.\n",
    "\n",
    "С помощью этого атрибута найдите сумму всех показателей важности, установите, какие два признака показывают наибольшую важность.\n",
    "\n",
    " \n",
    "\n",
    "print(clf.feature_importances_)\n",
    "[0.03211748 0.00154999 0.0070941  0.0011488  0.01436832 0.40270459\n",
    " 0.01424477 0.06403265 0.00496762 0.01169177 0.01808961 0.0123114\n",
    " 0.41567892]\n",
    "feature_importance = pd.DataFrame({'name':X.columns, \n",
    "                                   'feature_importance':clf.feature_importances_}, \n",
    "                                  columns=['feature_importance', 'name'])\n",
    "feature_importance\n",
    "feature_importance  name\n",
    "0   0.032117    CRIM\n",
    "1   0.001550    ZN\n",
    "2   0.007094    INDUS\n",
    "3   0.001149    CHAS\n",
    "4   0.014368    NOX\n",
    "5   0.402705    RM\n",
    "6   0.014245    AGE\n",
    "7   0.064033    DIS\n",
    "8   0.004968    RAD\n",
    "9   0.011692    TAX\n",
    "10  0.018090    PTRATIO\n",
    "11  0.012311    B\n",
    "12  0.415679    LSTAT\n",
    "feature_importance.nlargest(2, 'feature_importance')\n",
    "feature_importance  name\n",
    "12  0.415679    LSTAT\n",
    "5   0.402705    RM\n",
    "Признаки LSTAT и RM обладают наибольшей важностью.\n",
    "\n",
    "* Задание 4\n",
    "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.\n",
    "\n",
    "Для этого датасета мы будем решать задачу классификации - будем определять, какие из транзакциции по кредитной карте являются мошенническими.\n",
    "\n",
    "Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки), так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.\n",
    "\n",
    "Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
    "\n",
    "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n",
    "\n",
    "Загрузите датасет creditcard.csv и создайте датафрейм df.\n",
    "\n",
    "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована.\n",
    "\n",
    "Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.\n",
    "\n",
    "Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
    "\n",
    "pd.options.display.max_columns = 100.\n",
    "\n",
    "Просмотрите первые 10 строк датафрейма df.\n",
    "\n",
    "Создайте датафрейм X из датафрейма df, исключив столбец Class.\n",
    "\n",
    "Создайте объект Series под названием y из столбца Class.\n",
    "\n",
    "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.\n",
    "\n",
    "У вас должны получиться объекты X_train, X_test, y_train и y_test.\n",
    "\n",
    "Просмотрите информацию о их форме.\n",
    "\n",
    "Для поиска по сетке параметров задайте такие параметры:\n",
    "\n",
    "parameters = [{'n_estimators': [10, 15],\n",
    "\n",
    "'max_features': np.arange(3, 5),\n",
    "\n",
    "'max_depth': np.arange(4, 7)}]\n",
    "\n",
    "Создайте модель GridSearchCV со следующими аргументами:\n",
    "\n",
    "estimator=RandomForestClassifier(random_state=100),\n",
    "\n",
    "param_grid=parameters,\n",
    "\n",
    "scoring='roc_auc',\n",
    "\n",
    "cv=3.\n",
    "\n",
    "Обучите модель на тренировочном наборе данных (может занять несколько минут).\n",
    "\n",
    "Просмотрите параметры лучшей модели с помощью атрибута bestparams.\n",
    "\n",
    "Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
    "\n",
    "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba.\n",
    "\n",
    "Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n",
    "\n",
    "Вычислите AUC на тестовых данных и сравните с результатом, полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba.\n",
    "\n",
    "df = pd.read_csv('../Lesson04/creditcard.csv.zip', compression='zip')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "df['Class'].value_counts(normalize=True)\n",
    "0    0.998273\n",
    "1    0.001727\n",
    "Name: Class, dtype: float64\n",
    "df.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 284807 entries, 0 to 284806\n",
    "Data columns (total 31 columns):\n",
    "Time      284807 non-null float64\n",
    "V1        284807 non-null float64\n",
    "V2        284807 non-null float64\n",
    "V3        284807 non-null float64\n",
    "V4        284807 non-null float64\n",
    "V5        284807 non-null float64\n",
    "V6        284807 non-null float64\n",
    "V7        284807 non-null float64\n",
    "V8        284807 non-null float64\n",
    "V9        284807 non-null float64\n",
    "V10       284807 non-null float64\n",
    "V11       284807 non-null float64\n",
    "V12       284807 non-null float64\n",
    "V13       284807 non-null float64\n",
    "V14       284807 non-null float64\n",
    "V15       284807 non-null float64\n",
    "V16       284807 non-null float64\n",
    "V17       284807 non-null float64\n",
    "V18       284807 non-null float64\n",
    "V19       284807 non-null float64\n",
    "V20       284807 non-null float64\n",
    "V21       284807 non-null float64\n",
    "V22       284807 non-null float64\n",
    "V23       284807 non-null float64\n",
    "V24       284807 non-null float64\n",
    "V25       284807 non-null float64\n",
    "V26       284807 non-null float64\n",
    "V27       284807 non-null float64\n",
    "V28       284807 non-null float64\n",
    "Amount    284807 non-null float64\n",
    "Class     284807 non-null int64\n",
    "dtypes: float64(30), int64(1)\n",
    "memory usage: 67.4 MB\n",
    "pd.options.display.max_columns=100\n",
    "df.head(10)\n",
    "Time    V1  V2  V3  V4  V5  V6  V7  V8  V9  V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount  Class\n",
    "0   0.0 -1.359807   -0.072781   2.536347    1.378155    -0.338321   0.462388    0.239599    0.098698    0.363787    0.090794    -0.551600   -0.617801   -0.991390   -0.311169   1.468177    -0.470401   0.207971    0.025791    0.403993    0.251412    -0.018307   0.277838    -0.110474   0.066928    0.128539    -0.189115   0.133558    -0.021053   149.62  0\n",
    "1   0.0 1.191857    0.266151    0.166480    0.448154    0.060018    -0.082361   -0.078803   0.085102    -0.255425   -0.166974   1.612727    1.065235    0.489095    -0.143772   0.635558    0.463917    -0.114805   -0.183361   -0.145783   -0.069083   -0.225775   -0.638672   0.101288    -0.339846   0.167170    0.125895    -0.008983   0.014724    2.69    0\n",
    "2   1.0 -1.358354   -1.340163   1.773209    0.379780    -0.503198   1.800499    0.791461    0.247676    -1.514654   0.207643    0.624501    0.066084    0.717293    -0.165946   2.345865    -2.890083   1.109969    -0.121359   -2.261857   0.524980    0.247998    0.771679    0.909412    -0.689281   -0.327642   -0.139097   -0.055353   -0.059752   378.66  0\n",
    "3   1.0 -0.966272   -0.185226   1.792993    -0.863291   -0.010309   1.247203    0.237609    0.377436    -1.387024   -0.054952   -0.226487   0.178228    0.507757    -0.287924   -0.631418   -1.059647   -0.684093   1.965775    -1.232622   -0.208038   -0.108300   0.005274    -0.190321   -1.175575   0.647376    -0.221929   0.062723    0.061458    123.50  0\n",
    "4   2.0 -1.158233   0.877737    1.548718    0.403034    -0.407193   0.095921    0.592941    -0.270533   0.817739    0.753074    -0.822843   0.538196    1.345852    -1.119670   0.175121    -0.451449   -0.237033   -0.038195   0.803487    0.408542    -0.009431   0.798278    -0.137458   0.141267    -0.206010   0.502292    0.219422    0.215153    69.99   0\n",
    "5   2.0 -0.425966   0.960523    1.141109    -0.168252   0.420987    -0.029728   0.476201    0.260314    -0.568671   -0.371407   1.341262    0.359894    -0.358091   -0.137134   0.517617    0.401726    -0.058133   0.068653    -0.033194   0.084968    -0.208254   -0.559825   -0.026398   -0.371427   -0.232794   0.105915    0.253844    0.081080    3.67    0\n",
    "6   4.0 1.229658    0.141004    0.045371    1.202613    0.191881    0.272708    -0.005159   0.081213    0.464960    -0.099254   -1.416907   -0.153826   -0.751063   0.167372    0.050144    -0.443587   0.002821    -0.611987   -0.045575   -0.219633   -0.167716   -0.270710   -0.154104   -0.780055   0.750137    -0.257237   0.034507    0.005168    4.99    0\n",
    "7   7.0 -0.644269   1.417964    1.074380    -0.492199   0.948934    0.428118    1.120631    -3.807864   0.615375    1.249376    -0.619468   0.291474    1.757964    -1.323865   0.686133    -0.076127   -1.222127   -0.358222   0.324505    -0.156742   1.943465    -1.015455   0.057504    -0.649709   -0.415267   -0.051634   -1.206921   -1.085339   40.80   0\n",
    "8   7.0 -0.894286   0.286157    -0.113192   -0.271526   2.669599    3.721818    0.370145    0.851084    -0.392048   -0.410430   -0.705117   -0.110452   -0.286254   0.074355    -0.328783   -0.210077   -0.499768   0.118765    0.570328    0.052736    -0.073425   -0.268092   -0.204233   1.011592    0.373205    -0.384157   0.011747    0.142404    93.20   0\n",
    "9   9.0 -0.338262   1.119593    1.044367    -0.222187   0.499361    -0.246761   0.651583    0.069539    -0.736727   -0.366846   1.017614    0.836390    1.006844    -0.443523   0.150219    0.739453    -0.540980   0.476677    0.451773    0.203711    -0.246914   -0.633753   -0.120794   -0.385050   -0.069733   0.094199    0.246219    0.083076    3.68    0\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "print('X_train ', X_train.shape)\n",
    "print('X_test ', X_test.shape)\n",
    "print('y_train ', y_train.shape)\n",
    "print('y_test ', y_test.shape)\n",
    "X_train  (199364, 30)\n",
    "X_test  (85443, 30)\n",
    "y_train  (199364,)\n",
    "y_test  (85443,)\n",
    "parameters = [{\n",
    "    'n_estimators': [10, 15], \n",
    "    'max_features': np.arange(3, 5), \n",
    "    'max_depth': np.arange(4, 7)\n",
    "}] \n",
    "clf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=100),\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
    "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
    "                                              criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',\n",
    "                                              max_leaf_nodes=None,\n",
    "                                              min_impurity_decrease=0.0,\n",
    "                                              min_impurity_split=None,\n",
    "                                              min_samples_leaf=1,\n",
    "                                              min_samples_split=2,\n",
    "                                              min_weight_fraction_leaf=0.0,\n",
    "                                              n_estimators='warn', n_jobs=None,\n",
    "                                              oob_score=False, random_state=100,\n",
    "                                              verbose=0, warm_start=False),\n",
    "             iid='warn', n_jobs=None,\n",
    "             param_grid=[{'max_depth': array([4, 5, 6]),\n",
    "                          'max_features': array([3, 4]),\n",
    "                          'n_estimators': [10, 15]}],\n",
    "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
    "             scoring='roc_auc', verbose=0)\n",
    "clf.best_params_\n",
    "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}\n",
    "clf = RandomForestClassifier(max_depth=6, max_features=3, n_estimators=15)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=6, max_features=3, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=15,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_pred_proba = y_pred[:, 1]\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred_proba)\n",
    "0.9577549221065841\n",
    "*Дополнительные задания:\n",
    "Загрузите датасет Wine из встроенных датасетов sklearn.datasets с помощью функции load_wine в переменную data.\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "Полученный датасет не является датафреймом. Это структура данных, имеющая ключи аналогично словарю. Просмотрите тип данных этой структуры данных и создайте список data_keys, содержащий ее ключи.\n",
    "data_keys = data.keys()\n",
    "print(data_keys)\n",
    "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n",
    "Просмотрите данные, описание и названия признаков в датасете. Описание нужно вывести в виде привычного, аккуратно оформленного текста, без обозначений переноса строки, но с самими переносами и т.д.\n",
    "data.data\n",
    "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
    "        1.065e+03],\n",
    "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
    "        1.050e+03],\n",
    "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
    "        1.185e+03],\n",
    "       ...,\n",
    "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
    "        8.350e+02],\n",
    "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
    "        8.400e+02],\n",
    "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
    "        5.600e+02]])\n",
    "print(data.DESCR)\n",
    ".. _wine_dataset:\n",
    "\n",
    "Wine recognition dataset\n",
    "------------------------\n",
    "\n",
    "**Data Set Characteristics:**\n",
    "\n",
    "    :Number of Instances: 178 (50 in each of three classes)\n",
    "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
    "    :Attribute Information:\n",
    "        - Alcohol\n",
    "        - Malic acid\n",
    "        - Ash\n",
    "        - Alcalinity of ash  \n",
    "        - Magnesium\n",
    "        - Total phenols\n",
    "        - Flavanoids\n",
    "        - Nonflavanoid phenols\n",
    "        - Proanthocyanins\n",
    "        - Color intensity\n",
    "        - Hue\n",
    "        - OD280/OD315 of diluted wines\n",
    "        - Proline\n",
    "\n",
    "    - class:\n",
    "            - class_0\n",
    "            - class_1\n",
    "            - class_2\n",
    "        \n",
    "    :Summary Statistics:\n",
    "    \n",
    "    ============================= ==== ===== ======= =====\n",
    "                                   Min   Max   Mean     SD\n",
    "    ============================= ==== ===== ======= =====\n",
    "    Alcohol:                      11.0  14.8    13.0   0.8\n",
    "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
    "    Ash:                          1.36  3.23    2.36  0.27\n",
    "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
    "    Magnesium:                    70.0 162.0    99.7  14.3\n",
    "    Total Phenols:                0.98  3.88    2.29  0.63\n",
    "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
    "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
    "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
    "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
    "    Hue:                          0.48  1.71    0.96  0.23\n",
    "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
    "    Proline:                       278  1680     746   315\n",
    "    ============================= ==== ===== ======= =====\n",
    "\n",
    "    :Missing Attribute Values: None\n",
    "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
    "    :Creator: R.A. Fisher\n",
    "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
    "    :Date: July, 1988\n",
    "\n",
    "This is a copy of UCI ML Wine recognition datasets.\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
    "\n",
    "The data is the results of a chemical analysis of wines grown in the same\n",
    "region in Italy by three different cultivators. There are thirteen different\n",
    "measurements taken for different constituents found in the three types of\n",
    "wine.\n",
    "\n",
    "Original Owners: \n",
    "\n",
    "Forina, M. et al, PARVUS - \n",
    "An Extendible Package for Data Exploration, Classification and Correlation. \n",
    "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
    "Via Brigata Salerno, 16147 Genoa, Italy.\n",
    "\n",
    "Citation:\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository\n",
    "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
    "School of Information and Computer Science. \n",
    "\n",
    ".. topic:: References\n",
    "\n",
    "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
    "  Comparison of Classifiers in High Dimensional Settings, \n",
    "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
    "  Mathematics and Statistics, James Cook University of North Queensland. \n",
    "  (Also submitted to Technometrics). \n",
    "\n",
    "  The data was used with many others for comparing various \n",
    "  classifiers. The classes are separable, though only RDA \n",
    "  has achieved 100% correct classification. \n",
    "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
    "  (All results using the leave-one-out technique) \n",
    "\n",
    "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
    "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
    "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
    "  Mathematics and Statistics, James Cook University of North Queensland. \n",
    "  (Also submitted to Journal of Chemometrics).\n",
    "\n",
    "data.feature_names\n",
    "['alcohol',\n",
    " 'malic_acid',\n",
    " 'ash',\n",
    " 'alcalinity_of_ash',\n",
    " 'magnesium',\n",
    " 'total_phenols',\n",
    " 'flavanoids',\n",
    " 'nonflavanoid_phenols',\n",
    " 'proanthocyanins',\n",
    " 'color_intensity',\n",
    " 'hue',\n",
    " 'od280/od315_of_diluted_wines',\n",
    " 'proline']\n",
    "Сколько классов содержит целевая переменная датасета? Выведите названия классов.\n",
    "print(set(data.target))\n",
    "print(len(set(data.target)))\n",
    "{0, 1, 2}\n",
    "3\n",
    "data.target_names\n",
    "array(['class_0', 'class_1', 'class_2'], dtype='<U7')\n",
    "На основе данных датасета (они содержатся в двумерном массиве Numpy) и названий признаков создайте датафрейм под названием X.\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X.head()\n",
    "alcohol malic_acid  ash alcalinity_of_ash   magnesium   total_phenols   flavanoids  nonflavanoid_phenols    proanthocyanins color_intensity hue od280/od315_of_diluted_wines    proline\n",
    "0   14.23   1.71    2.43    15.6    127.0   2.80    3.06    0.28    2.29    5.64    1.04    3.92    1065.0\n",
    "1   13.20   1.78    2.14    11.2    100.0   2.65    2.76    0.26    1.28    4.38    1.05    3.40    1050.0\n",
    "2   13.16   2.36    2.67    18.6    101.0   2.80    3.24    0.30    2.81    5.68    1.03    3.17    1185.0\n",
    "3   14.37   1.95    2.50    16.8    113.0   3.85    3.49    0.24    2.18    7.80    0.86    3.45    1480.0\n",
    "4   13.24   2.59    2.87    21.0    118.0   2.80    2.69    0.39    1.82    4.32    1.04    2.93    735.0\n",
    "Выясните размер датафрейма X и установите, имеются ли в нем пропущенные значения.\n",
    "X.shape\n",
    "(178, 13)\n",
    "X.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 178 entries, 0 to 177\n",
    "Data columns (total 13 columns):\n",
    "alcohol                         178 non-null float64\n",
    "malic_acid                      178 non-null float64\n",
    "ash                             178 non-null float64\n",
    "alcalinity_of_ash               178 non-null float64\n",
    "magnesium                       178 non-null float64\n",
    "total_phenols                   178 non-null float64\n",
    "flavanoids                      178 non-null float64\n",
    "nonflavanoid_phenols            178 non-null float64\n",
    "proanthocyanins                 178 non-null float64\n",
    "color_intensity                 178 non-null float64\n",
    "hue                             178 non-null float64\n",
    "od280/od315_of_diluted_wines    178 non-null float64\n",
    "proline                         178 non-null float64\n",
    "dtypes: float64(13)\n",
    "memory usage: 18.2 KB\n",
    "Добавьте в датафрейм поле с классами вин в виде чисел, имеющих тип данных numpy.int64. Название поля - 'target'.\n",
    "X['target'] = data.target\n",
    "X.head()\n",
    "alcohol malic_acid  ash alcalinity_of_ash   magnesium   total_phenols   flavanoids  nonflavanoid_phenols    proanthocyanins color_intensity hue od280/od315_of_diluted_wines    proline target\n",
    "0   14.23   1.71    2.43    15.6    127.0   2.80    3.06    0.28    2.29    5.64    1.04    3.92    1065.0  0\n",
    "1   13.20   1.78    2.14    11.2    100.0   2.65    2.76    0.26    1.28    4.38    1.05    3.40    1050.0  0\n",
    "2   13.16   2.36    2.67    18.6    101.0   2.80    3.24    0.30    2.81    5.68    1.03    3.17    1185.0  0\n",
    "3   14.37   1.95    2.50    16.8    113.0   3.85    3.49    0.24    2.18    7.80    0.86    3.45    1480.0  0\n",
    "4   13.24   2.59    2.87    21.0    118.0   2.80    2.69    0.39    1.82    4.32    1.04    2.93    735.0   0\n",
    "Постройте матрицу корреляций для всех полей X. Дайте полученному датафрейму название X_corr.\n",
    "X_corr = X.corr()\n",
    "X_corr\n",
    "alcohol malic_acid  ash alcalinity_of_ash   magnesium   total_phenols   flavanoids  nonflavanoid_phenols    proanthocyanins color_intensity hue od280/od315_of_diluted_wines    proline target\n",
    "alcohol 1.000000    0.094397    0.211545    -0.310235   0.270798    0.289101    0.236815    -0.155929   0.136698    0.546364    -0.071747   0.072343    0.643720    -0.328222\n",
    "malic_acid  0.094397    1.000000    0.164045    0.288500    -0.054575   -0.335167   -0.411007   0.292977    -0.220746   0.248985    -0.561296   -0.368710   -0.192011   0.437776\n",
    "ash 0.211545    0.164045    1.000000    0.443367    0.286587    0.128980    0.115077    0.186230    0.009652    0.258887    -0.074667   0.003911    0.223626    -0.049643\n",
    "alcalinity_of_ash   -0.310235   0.288500    0.443367    1.000000    -0.083333   -0.321113   -0.351370   0.361922    -0.197327   0.018732    -0.273955   -0.276769   -0.440597   0.517859\n",
    "magnesium   0.270798    -0.054575   0.286587    -0.083333   1.000000    0.214401    0.195784    -0.256294   0.236441    0.199950    0.055398    0.066004    0.393351    -0.209179\n",
    "total_phenols   0.289101    -0.335167   0.128980    -0.321113   0.214401    1.000000    0.864564    -0.449935   0.612413    -0.055136   0.433681    0.699949    0.498115    -0.719163\n",
    "flavanoids  0.236815    -0.411007   0.115077    -0.351370   0.195784    0.864564    1.000000    -0.537900   0.652692    -0.172379   0.543479    0.787194    0.494193    -0.847498\n",
    "nonflavanoid_phenols    -0.155929   0.292977    0.186230    0.361922    -0.256294   -0.449935   -0.537900   1.000000    -0.365845   0.139057    -0.262640   -0.503270   -0.311385   0.489109\n",
    "proanthocyanins 0.136698    -0.220746   0.009652    -0.197327   0.236441    0.612413    0.652692    -0.365845   1.000000    -0.025250   0.295544    0.519067    0.330417    -0.499130\n",
    "color_intensity 0.546364    0.248985    0.258887    0.018732    0.199950    -0.055136   -0.172379   0.139057    -0.025250   1.000000    -0.521813   -0.428815   0.316100    0.265668\n",
    "hue -0.071747   -0.561296   -0.074667   -0.273955   0.055398    0.433681    0.543479    -0.262640   0.295544    -0.521813   1.000000    0.565468    0.236183    -0.617369\n",
    "od280/od315_of_diluted_wines    0.072343    -0.368710   0.003911    -0.276769   0.066004    0.699949    0.787194    -0.503270   0.519067    -0.428815   0.565468    1.000000    0.312761    -0.788230\n",
    "proline 0.643720    -0.192011   0.223626    -0.440597   0.393351    0.498115    0.494193    -0.311385   0.330417    0.316100    0.236183    0.312761    1.000000    -0.633717\n",
    "target  -0.328222   0.437776    -0.049643   0.517859    -0.209179   -0.719163   -0.847498   0.489109    -0.499130   0.265668    -0.617369   -0.788230   -0.633717   1.000000\n",
    "Создайте список high_corr из признаков, корреляция которых с полем target по абсолютному значению превышает 0.5 (причем, само поле target не должно входить в этот список).\n",
    "high_corr = X_corr.loc[(abs(X_corr['target']) > 0.5) & (X_corr.index != 'target'), X_corr.columns != 'target'].index\n",
    "high_corr\n",
    "Index(['alcalinity_of_ash', 'total_phenols', 'flavanoids', 'hue',\n",
    "       'od280/od315_of_diluted_wines', 'proline'],\n",
    "      dtype='object')\n",
    "Удалите из датафрейма X поле с целевой переменной. Для всех признаков, названия которых содержатся в списке high_corr, вычислите квадрат их значений и добавьте в датафрейм X соответствующие поля с суффиксом '_2', добавленного к первоначальному названию признака. Итоговый датафрейм должен содержать все поля, которые, были в нем изначально, а также поля с признаками из списка high_corr, возведенными в квадрат. Выведите описание полей датафрейма X с помощью метода describe.\n",
    "X = X.drop('target', axis=1)\n",
    "X.head()\n",
    "alcohol malic_acid  ash alcalinity_of_ash   magnesium   total_phenols   flavanoids  nonflavanoid_phenols    proanthocyanins color_intensity hue od280/od315_of_diluted_wines    proline\n",
    "0   14.23   1.71    2.43    15.6    127.0   2.80    3.06    0.28    2.29    5.64    1.04    3.92    1065.0\n",
    "1   13.20   1.78    2.14    11.2    100.0   2.65    2.76    0.26    1.28    4.38    1.05    3.40    1050.0\n",
    "2   13.16   2.36    2.67    18.6    101.0   2.80    3.24    0.30    2.81    5.68    1.03    3.17    1185.0\n",
    "3   14.37   1.95    2.50    16.8    113.0   3.85    3.49    0.24    2.18    7.80    0.86    3.45    1480.0\n",
    "4   13.24   2.59    2.87    21.0    118.0   2.80    2.69    0.39    1.82    4.32    1.04    2.93    735.0\n",
    "for feature_name in high_corr:\n",
    "    X[f'{feature_name}_2'] = X.apply(lambda row: row[feature_name] ** 2, axis=1)\n",
    "X.describe()\n",
    "alcohol malic_acid  ash alcalinity_of_ash   magnesium   total_phenols   flavanoids  nonflavanoid_phenols    proanthocyanins color_intensity hue od280/od315_of_diluted_wines    proline alcalinity_of_ash_2 total_phenols_2 flavanoids_2    hue_2   od280/od315_of_diluted_wines_2  proline_2\n",
    "count   178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  178.000000  1.780000e+02\n",
    "mean    13.000618   2.336348    2.366517    19.494944   99.741573   2.295112    2.029270    0.361854    1.590899    5.058090    0.957449    2.611685    746.893258  391.142865  5.657030    5.110049    0.968661    7.322155    6.564591e+05\n",
    "std 0.811827    1.117146    0.274344    3.339564    14.282484   0.625851    0.998859    0.124453    0.572359    2.318286    0.228572    0.709990    314.907474  133.671775  2.936294    4.211441    0.443798    3.584316    5.558591e+05\n",
    "min 11.030000   0.740000    1.360000    10.600000   70.000000   0.980000    0.340000    0.130000    0.410000    1.280000    0.480000    1.270000    278.000000  112.360000  0.960400    0.115600    0.230400    1.612900    7.728400e+04\n",
    "25% 12.362500   1.602500    2.210000    17.200000   88.000000   1.742500    1.205000    0.270000    1.250000    3.220000    0.782500    1.937500    500.500000  295.840000  3.036325    1.452100    0.612325    3.754075    2.505010e+05\n",
    "50% 13.050000   1.865000    2.360000    19.500000   98.000000   2.355000    2.135000    0.340000    1.555000    4.690000    0.965000    2.780000    673.500000  380.250000  5.546050    4.558250    0.931250    7.728400    4.536045e+05\n",
    "75% 13.677500   3.082500    2.557500    21.500000   107.000000  2.800000    2.875000    0.437500    1.950000    6.200000    1.120000    3.170000    985.000000  462.250000  7.840000    8.265700    1.254400    10.048900   9.702250e+05\n",
    "max 14.830000   5.800000    3.230000    30.000000   162.000000  3.880000    5.080000    0.660000    3.580000    13.000000   1.710000    4.000000    1680.000000 900.000000  15.054400   25.806400   2.924100    16.000000   2.822400e+06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
